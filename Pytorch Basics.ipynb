{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56f363f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n",
      "kagglecatsanddogs_5340/PetImages/Cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|██████▋                             | 2327/12501 [00:02<00:09, 1055.24it/s]Corrupt JPEG data: 214 extraneous bytes before marker 0xd9\n",
      " 70%|█████████████████████████           | 8702/12501 [00:08<00:03, 1064.62it/s]Corrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n",
      " 75%|██████████████████████████▉         | 9351/12501 [00:08<00:02, 1055.80it/s]Corrupt JPEG data: 99 extraneous bytes before marker 0xd9\n",
      " 86%|██████████████████████████████     | 10741/12501 [00:10<00:01, 1036.76it/s]Corrupt JPEG data: 128 extraneous bytes before marker 0xd9\n",
      " 90%|███████████████████████████████▌   | 11276/12501 [00:10<00:01, 1062.71it/s]Corrupt JPEG data: 239 extraneous bytes before marker 0xd9\n",
      "100%|███████████████████████████████████| 12501/12501 [00:11<00:00, 1051.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kagglecatsanddogs_5340/PetImages/Dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▌                                 | 1202/12501 [00:01<00:11, 996.74it/s]Corrupt JPEG data: 65 extraneous bytes before marker 0xd9\n",
      " 39%|██████████████▍                      | 4880/12501 [00:05<00:08, 951.97it/s]Corrupt JPEG data: 226 extraneous bytes before marker 0xd9\n",
      " 55%|████████████████████▌                | 6933/12501 [00:07<00:05, 956.74it/s]Corrupt JPEG data: 162 extraneous bytes before marker 0xd9\n",
      " 59%|█████████████████████▉               | 7414/12501 [00:07<00:05, 948.77it/s]Warning: unknown JFIF revision number 0.00\n",
      " 74%|██████████████████████████▊         | 9308/12501 [00:09<00:03, 1004.02it/s]Corrupt JPEG data: 2230 extraneous bytes before marker 0xd9\n",
      " 84%|██████████████████████████████▏     | 10492/12501 [00:10<00:02, 930.69it/s]Corrupt JPEG data: 254 extraneous bytes before marker 0xd9\n",
      " 91%|████████████████████████████████▊   | 11383/12501 [00:11<00:01, 984.39it/s]Corrupt JPEG data: 399 extraneous bytes before marker 0xd9\n",
      " 97%|██████████████████████████████████▊ | 12083/12501 [00:12<00:00, 994.39it/s]Corrupt JPEG data: 1403 extraneous bytes before marker 0xd9\n",
      "100%|████████████████████████████████████| 12501/12501 [00:12<00:00, 967.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cats: 12476\n",
      "Dogs: 12470\n",
      "10000\n",
      "1000\n",
      "9000 HEll\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "REBUILD_DATA = True # set to true to one once, then back to false unless you want to change something in your training data.\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 100\n",
    "    CATS = \"kagglecatsanddogs_5340/PetImages/Cat\"\n",
    "    DOGS = \"kagglecatsanddogs_5340/PetImages/Dog\"\n",
    "    TESTING = \"kagglecatsanddogs_5340/PetImages/Testing\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])  # do something like print(np.eye(2)[1]), just makes one_hot \n",
    "                        #print(np.eye(2)[self.LABELS[label]])\n",
    "\n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        #print(label, f, str(e))\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data1.npy\", self.training_data)\n",
    "        print('Cats:',dogsvcats.catcount)\n",
    "        print('Dogs:',dogsvcats.dogcount)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "\n",
    "        x = torch.randn(100,100).view(-1,1,100,100)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "    def convs(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2,2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net = Net().to(device)\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()\n",
    "\n",
    "training_data = np.load(\"training_data1.npy\", allow_pickle=True)\n",
    "training_data = training_data[:10000]\n",
    "print(len(training_data))\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1, 100, 100)\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "VAL_PCT = 0.1\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "print(val_size)\n",
    "\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "print(len(train_X),\"HEll\")\n",
    "print(len(test_X))\n",
    "\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 3\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,100,100)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            net.zero_grad()\n",
    "            outputs = net(batch_X)\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(loss)\n",
    "\n",
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_X))):\n",
    "            real_class = torch.argmax(test_y[i]).to(device)\n",
    "            net_out = net(test_X[i].view(-1, 1, 100, 100).to(device))[0]\n",
    "\n",
    "            predicted_class = torch.argmax(net_out)\n",
    "            if predicted_class == real_class:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    print(\"Accuracy:\", round(correct/total,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1709ee8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [04:47<00:00,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
      "In-sample acc: 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [04:47<00:00,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "In-sample acc: 0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 90/90 [04:46<00:00,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2501, grad_fn=<MseLossBackward0>)\n",
      "In-sample acc: 0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = net.to(device)\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 3\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,100,100)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            net.zero_grad()\n",
    "            outputs = net(batch_X)\n",
    "\n",
    "            matches  = [torch.argmax(i)==torch.argmax(j) for i, j in zip(outputs, batch_y)]\n",
    "            in_sample_acc = matches.count(True)/len(matches)\n",
    "\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(loss)\n",
    "        print(\"In-sample acc:\",round(in_sample_acc, 2))\n",
    "\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74d48cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.54\n"
     ]
    }
   ],
   "source": [
    "def batch_test(net):\n",
    "    BATCH_SIZE = 100\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        #np.random.shuffle(test_X)\n",
    "        #np.random.shuffle(test_y)\n",
    "\n",
    "        batch_X = test_X[:BATCH_SIZE].view(-1,1,100,100)\n",
    "        batch_y = test_y[:BATCH_SIZE]\n",
    "\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        net.zero_grad()\n",
    "        outputs = net(batch_X)\n",
    "\n",
    "        matches  = [torch.argmax(i)==torch.argmax(j) for i, j in zip(outputs, batch_y)]\n",
    "        acc = matches.count(True)/len(matches)\n",
    "\n",
    "        print(\"Test Accuracy:\", round(acc, 3))\n",
    "\n",
    "\n",
    "batch_test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfca097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd24a9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90fd1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "400ea5c9",
   "metadata": {},
   "source": [
    "### Pytorch Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f0195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fb30df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a496620",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.13.0)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.14.0)\n",
      "Requirement already satisfied: torchaudio in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.13.0)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->torchvision) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->torchvision) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->torchvision) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64a43aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy doesn't run on gpu\n",
    "# nice gpu have 1000's of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "850f9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b352b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  3.])\n"
     ]
    }
   ],
   "source": [
    "x=torch.Tensor([5,3])\n",
    "y=torch.Tensor([2,1])\n",
    "\n",
    "print(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d65ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.zeros([5,3])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae0d51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e11a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1615, 0.6722, 0.2501, 0.9883, 0.4283],\n",
       "        [0.8048, 0.5528, 0.1330, 0.2308, 0.1787]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=torch.rand([2,5])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f7a6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just like numpy but for gpu prep many methods are quite different lets see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ffb081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# like reshape has changed to view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b94ea1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to flatten in first to reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d9d3233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=y.view([1,10])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "000968e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1615, 0.6722, 0.2501, 0.9883, 0.4283, 0.8048, 0.5528, 0.1330, 0.2308,\n",
       "         0.1787]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c728d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.14.0)\r\n",
      "Requirement already satisfied: torch==1.13.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (1.13.0)\r\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (2.28.1)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (9.2.0)\r\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (1.21.6)\r\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (4.3.0)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->torchvision) (2.1.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->torchvision) (2022.6.15)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->torchvision) (1.26.12)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->torchvision) (3.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edb91181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abd24f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\", train=False, download=True, transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb1d5b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train,batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test,batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "376d3f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([8, 9, 1, 7, 4, 7, 1, 3, 6, 2])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print((data))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97ef3523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y =data[0][0], data[1][0]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78bd08fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5240033b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1ba4083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[0][0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dff2105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5923,\n",
       " 1: 6742,\n",
       " 2: 5958,\n",
       " 3: 6131,\n",
       " 4: 5842,\n",
       " 5: 5421,\n",
       " 6: 5918,\n",
       " 7: 6265,\n",
       " 8: 5851,\n",
       " 9: 5949}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total=0\n",
    "counter_dict={0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0}\n",
    "\n",
    "for data in trainset:\n",
    "    xs,ys= data\n",
    "    for j in ys:\n",
    "        counter_dict[int(j)]+=1\n",
    "\n",
    "counter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ff6dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "feeaa093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1=nn.Linear(28*28,64)\n",
    "        self.fc2=nn.Linear(64,64)\n",
    "        self.fc3=nn.Linear(64,64)\n",
    "        self.fc4=nn.Linear(64,10)\n",
    " # activation function run on the resulting output\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = (self.fc4(x))     \n",
    "        return(F.log_softmax(x, dim=1))\n",
    "        \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dfb8429",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))\n",
    "X=X.view(1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1808d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e348e219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3210, -2.2873, -2.2866, -2.2771, -2.4156, -2.3391, -2.1921, -2.3273,\n",
       "         -2.1803, -2.4293]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c3d20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f94f52ab",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2125, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0360, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ld/1ljb1qfd7yz5vk2y_hjdgmlm0000gp/T/ipykernel_1026/2354141709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    250\u001b[0m                  \u001b[0mfused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fused'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                  \u001b[0mgrad_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                  found_inf=found_inf)\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    314\u001b[0m          \u001b[0mdifferentiable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdifferentiable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m          \u001b[0mgrad_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m          found_inf=found_inf)\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer= optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS=15\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        X,y=data\n",
    "        net.zero_grad()      \n",
    "        output = net(X.view(-1,28*28))\n",
    "        loss = F.nll_loss(output,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d8bbc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.103\n"
     ]
    }
   ],
   "source": [
    "correct =0\n",
    "total =0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X,y = data\n",
    "        output = net(x.view(-1,(784)))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) ==y[idx]:\n",
    "                     correct+=1\n",
    "            total += 1\n",
    "                    \n",
    "print(round(correct/total,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5802ce14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79b76a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN PERforming better than rnn in sequential type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "760c9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dog and Cat Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f19333da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4316c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
